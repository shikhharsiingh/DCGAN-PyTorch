import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as T
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

#Hyperparameters
seed = 999
dataroot = "datasets/"
workers = 2
batch_size = 128
img_dim = 28
channels = 1
latentz = 100
num_epochs = 5
lr = 2e-4
beta1 = 0.5
gpus = 1 if torch.cuda.is_available() else 0
nfg = 28
nfd = 64
device = torch.device("cuda:0" if gpus else "cpu")

#Pre requisites
criterion = nn.BCELoss()

fixed_noise = torch.randn(64, latentz, 1, 1, device = device)

real_target = 1.
fake_target = 0.

transforms = T.Compose([
    T.Resize(img_dim),
    T.CenterCrop(img_dim),
    T.ToTensor(),
    T.Normalize(mean = 0.5, std = 0.5),
])

dataset = dset.MNIST(root = dataroot, transform = transforms, download = True)

dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, shuffle = True)

#Plotting some training images
batch = next(iter(dataloader))
plt.figure(figsize = (8, 8))
plt.axis("off")
plt.title("Training Images")
plt.imshow(np.transpose(vutils.make_grid(batch[0].to(device)[:64], padding = 2, normalize = True).cpu(), (1, 2, 0)))
#plt.show()         #If using on VSCode, uncomment to retain the plot

#Weights initialization from Normal Distribution, mean = 0, Standard deviation = 0.02
def init_weights(model):
    classname = model.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(model.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(model.weight.data, 1.0, 0.02)
        nn.init.constant_(model.bias.data, 0.0)

def conv_block_gen(in_channels, out_channels, kernel_size, stride, padding, bias = True):
    layers = nn.Sequential(
        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(True),
    )
    return layers

def conv_bloc_dis(in_channels, out_channels, kernel_size, stride, padding, bias = True, alpha = 0.2):
    layers = nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias),
        nn.BatchNorm2d(out_channels),
        nn.LeakyReLU(alpha, inplace = True)
    )
    return layers

#The generator constructs an image from an arbitrary input space
class Generator(nn.Module):
    def __init__(self, gpus):
        super(Generator, self).__init__()
        self.gpus = gpus
        self.layers = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d(latentz, nfg * 2, 7, 1, 0, bias=False),
            nn.BatchNorm2d(nfg * 2),
            nn.ReLU(True),
            # state size. (nfg * 2) x 7 x 7
            nn.ConvTranspose2d( nfg * 2, nfg, 4, 2, 1, bias=False),
            nn.BatchNorm2d(nfg),
            nn.ReLU(True),
            # state size. (nfg) x 14 x 14
            nn.ConvTranspose2d(nfg, channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (channels) x 28 x 28
        )

    def forward(self, x):
        return self.layers(x)

#Discriminator classifies the image generated by the Generator as fake or real
class Discriminator(nn.Module):
    def __init__(self, gpus):
        super(Discriminator, self).__init__()
        self.gpus = gpus
        self.layers = nn.Sequential(
            # input is (nc) x 28 x 28
            nn.Conv2d(channels, nfd, 3, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (nfd) x 14 x 14
            nn.Conv2d(nfd, nfd * 2, 3, 2, 1, bias=False),
            nn.BatchNorm2d(nfd * 2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(nfd * 2, 1, 7, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.layers(x)

def Create_DCObject(DCstring, gpus):
    DC = None
    if DCstring.lower() == 'gen':
        DC = Generator(gpus).to(device)
    elif DCstring.lower() == 'dis':
        DC = Discriminator(gpus).to(device)
    else: return None

    if (device.type == 'cuda') and (gpus > 1):
        DC = nn.DataParallel(DC, list(range(gpus)))
    
    DC.apply(init_weights)
    return DC

def plot_prog(G_losses, D_losses):
    plt.figure(figsize=(10,5))
    plt.title("Generator and Discriminator Loss During Training")
    plt.plot(G_losses,label="G")
    plt.plot(D_losses,label="D")
    plt.xlabel("iterations")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

DCGen = Create_DCObject('gen', gpus)
DCDis = Create_DCObject('dis', gpus)

print(DCGen)          #Uncomment to view the Generator structure
print(DCDis)          #Uncomment to view the Discriminator structure

optimizerG = optim.Adam(DCGen.parameters(), lr = lr, betas = (beta1, 0.999))
optimizerD = optim.Adam(DCDis.parameters(), lr = lr, betas = (beta1, 0.999))

#Training Loop

img_list = []
G_losses = []
D_losses = []
iters = 0


for epoch in range(num_epochs):
    for i, batch in enumerate(dataloader, 0):   #batch.shape = [2,]

        #First, we compute the loss of the discriminator's ability to classify all the data images as real
        DCDis.zero_grad()
        features = batch[0].to(device)          #shape = [128, 1, 28, 28]
        b_size = features.size(0)               #b_size = 128

        r_target = torch.full((b_size, ), real_target, dtype = torch.float).to(device) #shape = 128
        r_output = DCDis(features).view(-1)         #Sigmoid output of shape [128]

        real_loss = criterion(r_output, r_target)   #BCELoss on preds and targets
        real_loss.backward()                        #Backpropagation
        D_r = r_output.mean().item()                #Computing gradients

        #Now, we give it a noise, generate fake images from the noise using the generator and train the discriminator to classify them as real or fake
        noise = torch.randn(b_size, latentz, 1, 1, device = device)

        fake = DCGen(noise)
        f_target = torch.full((b_size, ), fake_target, dtype = torch.float, device = device)
        f_output = DCDis(fake.detach()).view(-1) #why did we use detach? this is because while backprop. everything will get changed which we do not want to. We do not want to change the fake predictions

        fake_loss = criterion(f_output, f_target)
        fake_loss.backward()
        D_G_z1 = f_output.mean().item()

        lossD = real_loss + fake_loss
        optimizerD.step()

        #Now we work upon the Generator
        DCGen.zero_grad()

        output = DCDis(fake).view(-1)
        lossG = criterion(output, r_target)
        lossG.backward()
        D_G_z2 = output.mean().item()

        optimizerG.step()

        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'% (epoch, num_epochs, i, len(dataloader),lossD.item(), lossG.item(), D_r, D_G_z1, D_G_z2))

        #For plotting the graphs, we store the losses of both the models
        G_losses.append(lossG.item())
        D_losses.append(lossD.item())

        if (iters % 500 == 0) or ((epoch == num_epochs - 1) and i == len(dataloader) - 1):
            with torch.no_grad():
                fake = DCGen(fixed_noise).detach().cpu()
            img_list.append(vutils.make_grid(fake, padding = 2, normalize = True))

        iters += 1

plot_prog(G_losses, D_losses)